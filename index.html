<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link rel="stylesheet" href="./style.css">
    <script type="module">

/********************************************************************
 // Utilities
********************************************************************/

function calculateAverage(array) {
    var total = 0;
    var count = 0;

    array.forEach(function(item, index) {
        total += item;
        count++;
    });

    return total / count;
}

/********************************************************************
 // Gesture Detector
********************************************************************/

import {
    GestureRecognizer,
    FilesetResolver,
    DrawingUtils
  } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";
  
const demosSection = document.getElementById("demos");
let gestureRecognizer;
let runningMode = "IMAGE";
let enableWebcamButton;
let webcamRunning = false;
const videoHeight = "360px";
const videoWidth = "480px";

const video = document.getElementById("webcam");
const canvasElement = document.getElementById("output_canvas");
const canvasCtx = canvasElement.getContext("2d");
const gestureOutput = document.getElementById("gesture_output");

// Before we can use HandLandmarker class we must wait for it to finish
// loading. Machine Learning models can be large and take a moment to
// get everything needed to run.
const createGestureRecognizer = async () => {
    const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
    );
    gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
        baseOptions: {
            modelAssetPath:
            "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
            delegate: "GPU"
        },
        runningMode: runningMode
    });
    demosSection.classList.remove("invisible");
};
createGestureRecognizer();

/********************************************************************
 // Webcam
********************************************************************/

// Check if webcam access is supported.
function hasGetUserMedia() {
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
}

// If webcam supported, add event listener to button for when user
// wants to activate it.
if (hasGetUserMedia()) {
    enableWebcamButton = document.getElementById("webcamButton");
    enableWebcamButton.addEventListener("click", enableCam);
} else {
    console.warn("getUserMedia() is not supported by your browser");
}

// Enable the live webcam view and start detection.
function enableCam(event) {
    if (!gestureRecognizer) {
        alert("Please wait for gestureRecognizer to load");
        return;
    }

    if (webcamRunning === true) {
        webcamRunning = false;
        enableWebcamButton.innerText = "ENABLE PREDICTIONS";
    } else {
        webcamRunning = true;
        enableWebcamButton.innerText = "DISABLE PREDICTIONS";
    }

    // getUsermedia parameters.
    const constraints = {
        video: true
    };

    // Activate the webcam stream.
    navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {
        video.srcObject = stream;
        video.addEventListener("loadeddata", predictWebcam);
    });
}

let lastVideoTime = -1;
let results = undefined;
async function predictWebcam() {
    const webcamElement = document.getElementById("webcam");
    // Now let's start detecting the stream.
    if (runningMode === "IMAGE") {
        runningMode = "VIDEO";
        await gestureRecognizer.setOptions({ runningMode: "VIDEO" });
    }
    
    let nowInMs = Date.now();
    if (video.currentTime !== lastVideoTime) {
        lastVideoTime = video.currentTime;
        results = gestureRecognizer.recognizeForVideo(video, nowInMs); // analyse video and recognize gestures
    }

    canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

    canvasElement.style.height = videoHeight;
    webcamElement.style.height = videoHeight;
    canvasElement.style.width = videoWidth;
    webcamElement.style.width = videoWidth;
    
    if (results.landmarks) {
        for (const landmarks of results.landmarks) {
            drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {
                color: "#00FF00",
                lineWidth: 5
            });
            drawLandmarks(canvasCtx, landmarks, { color: "#FF0000", lineWidth: 2 });
        }

        // Hier müsste man dann die Höhe der landmarks berechnen
    }

    
    canvasCtx.restore();

    if (results.gestures.length > 0) { 
        // parse information
        const categoryName = results.gestures[0][0].categoryName;
        const categoryScore = parseFloat(
            results.gestures[0][0].score * 100
        ).toFixed(2);

        // which hand 
        if (results.handednesses[0][0].index == 0) { // right
            const landmarks = results.landmarks[0];
            let landmarksX = landmarks.map(element => element.x);
            let landmarksY = landmarks.map(element => element.y);
            var rightX = calculateAverage(landmarksX)
            var rightY = calculateAverage(landmarksY)
            console.log("right Hand coordinates: ", rightX, rightY);
        } else if (results.handednesses[0][0].index == 1) { // left
            const landmarks = results.landmarks[0];
            let landmarksX = landmarks.map(element => element.x);
            let landmarksY = landmarks.map(element => element.y);
            var leftX = calculateAverage(landmarksX)
            var leftY = calculateAverage(landmarksY)
            console.log("left Hand coordinates: ", leftX, leftY);
        }


        // print the result to console
        // console.log(results, categoryName, categoryScore);

        // parse information and generate midi
        generateMidi(categoryName, categoryScore);

        // change the html interface
        gestureOutput.style.display = "block";
        gestureOutput.style.width = videoWidth;
        gestureOutput.innerText = `GestureRecognizer: ${categoryName}\n Confidence: ${categoryScore} %`;
    } else {
        gestureOutput.style.display = "none";
    }
    // Call this function again to keep predicting when the browser is ready.
    if (webcamRunning === true) {
        window.requestAnimationFrame(predictWebcam);
    }
}
  
/********************************************************************
 // Midi
********************************************************************/

// ############ Zustand ############
const idleChannel = 0; // Channel ohne Ton
let currentChannel = idleChannel; // aktueller Channel/Gestes (am Anfang idleChannel)
let midiOut = []; // Midi Outputs (werden in initDevices gesetzt)

// ############ Funktionen ############

function midiReady(midi) {
    // Also react to device changes.
    midi.addEventListener('statechange', (event) => initDevices(event.target));
    initDevices(midi); // see the next section!
}

function initDevices(midi) {
    // MIDI devices that you send data to.
    const outputs = midi.outputs.values();
    for (let output = outputs.next(); output && !output.done; output = outputs.next()) {
        midiOut.push(output.value);
    }
}

function sendMidiMessage(channel, pitch, velocity) {
    const noteOnMessage = [0x90, pitch, velocity];
    const noteOffMessage = [0x80, pitch, velocity];

    // Bei channel Wechsel / andere Geste
    if(channel != currentChannel){
        // Wenn aktuell ein Ton spielt, diesen stoppen
        if(currentChannel != idleChannel){
            const currentDevice = midiOut[currentChannel];
            currentDevice.send(noteOffMessage);
        }

        // Neuen Ton spielen
        const device = midiOut[channel];
        device.send(noteOnMessage);

        // Channel wechseln
        currentChannel = channel;
    }
}

// Übersetze Geste in Midi Channel und sende diese an sendMidiChannel
function generateMidi(gestureName, gestureConfidence) {
    const gestureToChannelMap = {
        'Closed_Fist': idleChannel,
        'Open_Palm': 1,
        'Pointing_Up': 3,
        'Thumb_Down': 4,
        'Thumb_Up': 5,
        'Victory': 6,
        'ILoveYou': 7,
    };

    const channel = gestureToChannelMap[gestureName] || idleChannel; // Channel der Geste
    let pitch = 50; // 
    let velocity = 100; // Später eventuell: y coordinate

    // send midi
    sendMidiMessage(channel, pitch, velocity);
}

/********************************************************************
 // Main
********************************************************************/

// Wird ausgefuehrt, wenn die Seite geladen ist
window.onload = function() {
    window.navigator.requestMIDIAccess()
    .then(
        (midi) => midiReady(midi),
        (err) => console.log('Something went wrong', err)
    );
}
    </script>

    <link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
    <script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>

</head>
<body>


        <h2><br>Demo: Webcam continuous hand gesture detection</h2>
        <p>Use your hand to make gestures in front of the camera to get gesture classification. </br>Click <b>enable webcam</b> below and grant access to the webcam if prompted.</p>

        <div id="liveView" class="videoView">
            <button id="webcamButton" class="mdc-button mdc-button--raised">
            <span class="mdc-button__ripple"></span>
            <span class="mdc-button__label">ENABLE WEBCAM</span>
            </button>
            <div style="position: relative;">
            <video id="webcam" autoplay playsinline></video>
            <canvas class="output_canvas" id="output_canvas" width="1280" height="720" style="position: absolute; left: 0px; top: 0px;"></canvas>
            <p id='gesture_output' class="output">
            </div>
        </div>
    </section>
</body>
</html>
